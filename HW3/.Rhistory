spc_steward_health_probs
spc_steward_health_probs %>% dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health_score) -> spc_steward_health_long
spc_steward_health_long
(spc_steward_health_long %>% mutate(val = str_replace(val, "pct_", "")) -> spc_steward_health_long )
spc_steward_health_long  %>%
ggplot(aes(fill=as.factor(health_score), x=val, y = pct )) +
geom_bar(position="dodge", stat="identity") +
facet_wrap(~spc_common, ncol=4 )
(x1 = df %>% filter(health != '', spc_common != '', status=='Alive')  %>% dplyr::select(tree_id, borough, status, health, spc_common, steward) %>% group_by(spc_common, borough, health , steward) %>% summarize( ct = n()) )
head(x1)
# Borough View
(bv = x1 %>% group_by(spc_common, borough) %>% summarize( ct = sum(ct)) %>% pivot_wider(names_from = borough, values_from = ct, values_fill = 0 ) )
bv %>% pivot_longer(!spc_common, names_to = "borough", values_to = "ct") %>% ggplot(aes(x = borough, y = spc_common, fill = log(ct) )) +
geom_tile() + scale_fill_viridis_c() +
geom_text(aes(label=ct) )
(x2 = x1 %>% group_by(borough, spc_common, health ) %>% summarize( ct2 = sum(ct)) %>% group_by(borough, spc_common) %>% mutate( sct2 = sum(ct2) , pct = ct2 / sct2 ) )
my_spc = 'American hornbeam'
x2 %>% filter(spc_common == my_spc) %>% ungroup() %>% dplyr::select(borough, health, pct ) %>%
pivot_wider(names_from = health, values_from = pct, values_fill = 0)
df %>% filter(health != '' , status =='Alive', spc_common != '')
df %>% filter(health != '' , status =='Alive', spc_common != '') -> dff
unique(dff$spc_common)
View(t(head(dff)))
spc_steward_health_long  %>%
ggplot(aes(fill=val, x=as.factor(head_score), y = pct )) +
geom_bar(position="dodge", stat="identity") +
facet_wrap(~spc_common, ncol=4 )
spc_steward_health_long  %>%
ggplot(aes(fill=val, x=as.factor(health_score), y = pct )) +
geom_bar(position="dodge", stat="identity") +
facet_wrap(~spc_common, ncol=4 )
by_spc_health_steward %>% left_join( spc_steward_marginals, by = c("spc_common", "steward")) %>% mutate( health_score=ifelse(health=="Good", 2, ifelse(health=="Fair", 1, 0 )), pct =  ct / val) %>% ungroup('health') %>% dplyr::select( spc_common, health_score, steward, ct, pct) %>% pivot_wider(names_from = "steward", values_from = c("ct", "pct"), values_fill = 0 )   %>% arrange(spc_common, health_score) %>%
dplyr::select( spc_common, health_score,  ct_None, pct_None, ct_1or2, pct_1or2, ct_3or4, pct_3or4, ct_4orMore, pct_4orMore) ->
spc_steward_health_probs_full
# Use this probs_full to store the health as text string.
spc_steward_health_probs_full %>% select( -health) -> spc_steward_health_probs
by_spc_health_steward %>% left_join( spc_steward_marginals, by = c("spc_common", "steward")) %>% mutate( health_score=ifelse(health=="Good", 2, ifelse(health=="Fair", 1, 0 )), pct =  ct / val) %>% ungroup('health') %>% dplyr::select( spc_common, health_score, steward, ct, pct) %>% pivot_wider(names_from = "steward", values_from = c("ct", "pct"), values_fill = 0 )   %>% arrange(spc_common, health_score) %>%
dplyr::select( spc_common, health, health_score,  ct_None, pct_None, ct_1or2, pct_1or2, ct_3or4, pct_3or4, ct_4orMore, pct_4orMore) ->
spc_steward_health_probs_full
by_spc_health_steward %>% left_join( spc_steward_marginals, by = c("spc_common", "steward")) %>% mutate( health_score=ifelse(health=="Good", 2, ifelse(health=="Fair", 1, 0 )), pct =  ct / val) %>% ungroup('health') %>% dplyr::select( spc_common, health, health_score, steward, ct, pct) %>% pivot_wider(names_from = "steward", values_from = c("ct", "pct"), values_fill = 0 )   %>% arrange(spc_common, health_score) %>%
dplyr::select( spc_common, health, health_score,  ct_None, pct_None, ct_1or2, pct_1or2, ct_3or4, pct_3or4, ct_4orMore, pct_4orMore) ->
spc_steward_health_probs_full
# Use this probs_full to store the health as text string.
spc_steward_health_probs_full %>% select( -health) -> spc_steward_health_probs
spc_steward_health_probs
head(spc_steward_health_probs_full)
head(spc_steward_health_probs_full)
spc_steward_health_probs_full %>% select(-health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health_score)
by_spc_health_steward %>% left_join( spc_steward_marginals, by = c("spc_common", "steward")) %>% mutate( health_score=ifelse(health=="Good", 2, ifelse(health=="Fair", 1, 0 )), pct =  ct / val) %>% ungroup('health') %>% dplyr::select( spc_common, health, health_score, steward, ct, pct) %>% pivot_wider(names_from = "steward", values_from = c("ct", "pct"), values_fill = 0 )   %>% arrange(spc_common, health_score) %>%
dplyr::select( spc_common, health, health_score,  ct_None, pct_None, ct_1or2, pct_1or2, ct_3or4, pct_3or4, ct_4orMore, pct_4orMore) ->
spc_steward_health_probs_full
# Use this probs_full to store the health as text string.
spc_steward_health_probs_full %>% select( -health) -> spc_steward_health_probs
spc_steward_health_probs
head(spc_steward_health_probs_full)
spc_steward_health_probs_full %>% select(-health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health_score)
head(spc_steward_health_probs_full)
spc_steward_health_probs_full %>% select(-health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health_score)
spc_steward_health_probs_full %>% select(- health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health_score)
spc_steward_health_probs_full %>% dplyr::select(-health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health_score)
spc_steward_health_probs_full$health_score
spc_steward_health_probs_full %>%
dplyr::select(!starts_with("ct_")) %>%
dplyr::select(!starts_with("health_s")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health_score)
spc_steward_health_probs_full %>%
dplyr::select( -health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health)
spc_steward_health_probs_full %>%
dplyr::select( -health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health) %>%
ggplot(aes(fill=val, x=health, y = ct )) +
geom_bar(position="dodge", stat="identity") +
facet_wrap(~spc_common, ncol=4 )
spc_steward_health_probs_full %>%
dplyr::select( -health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
arrange(spc_common, val, health) %>%
ggplot(aes(fill=val, x=health, y = pct )) +
geom_bar(position="dodge", stat="identity") +
facet_wrap(~spc_common, ncol=4 )
spc_steward_health_probs_full %>%
dplyr::select( -health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" )
spc_steward_health_probs_full %>%
dplyr::select( -health_score) %>%
dplyr::select(!starts_with("ct_")) %>%
pivot_longer( cols=starts_with("pct_"), names_to = "val", values_to = "pct" ) %>%
mutate(val = str_replace(val, "pct_", "")) %>%
arrange(spc_common, val, health) %>%
ggplot(aes(fill=val, x=health, y = pct )) +
geom_bar(position="dodge", stat="identity") +
facet_wrap(~spc_common, ncol=4 )
df$health == ""
df[df$health == ""]
df[,df$health == ""]
df[df$health == ""]
df
df %>% select(tree_id, spc_common, health, status, address )
df %>% select(tree_id, spc_common, health, status, address , nta_name )
df %>% select(tree_id, spc_common, health, status, address , nta_name ) %>% filter( spc_common == '' | health == '')
my_spc = 'American hornbeam'
x2 %>% filter(spc_common == my_spc) %>% ungroup() %>% dplyr::select(borough, health, pct ) %>%
pivot_wider(names_from = health, values_from = pct, values_fill = 0)
df %>% filter(tree_id == 211205)
df %>% filter(tree_id == 211205) %>% t()
df
df %>% filter(spc_common != '')
df %>% filter(spc_common != '' & health != '' )
df %>% filter(spc_common != '' & health != '' ) %>% filter(spc_common=="American elm")
df %>% filter(spc_common != '' & health != '' ) %>% filter(spc_common=="American elm") %>% group_by(borough, health ) %>% summarize(ct = n())
View(spc_steward_health_probs_full)
View(spc_steward_health_long)
View(spc_steward_health_probs)
View(status_by_nta)
View(spc_steward_marginals)
View(spc_steward_health_probs_full)
View(spc_steward_health_probs)
View(spc_steward_health_long)
View(by_spc_health_steward)
View(x1)
View(x2)
View(bv)
bv %>% filter(Bronx == 0 )
bv %>% filter(Brooklyn == 0 )
bv %>% filter(Manhattan == 0 )
bv %>% filter(Queens == 0 )
bv %>% filter(`Staten Island` == 0 )
76/12
76 div 12
76 mod 12
76 // 12
76 % 12
76 %% 12
76 // 12
76 /12
64 * 3
192 / 12
192 %% 12
215 - 192
1400 * 3
x0 = 4020416.18
x1 = 4072437.59
x2 = 4608243.73
x2 - x0
x1 - x0
x2 - x1
x2 - x0
(x2-x0)/x0
(x2-x1)/x1
x2 * (1.06)^10
x2 * (1.06)^15
x2 * (1.07)^15
x2 * (1.07)^10
x2 * (1.05)
x2 * 1.05
x2 * 1.06
x2 * 1.10
x2 + 200000 + 600000
x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02
(x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02) * 1.06
(x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02) * 1.10
(x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02) * (1.06)^6
(x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02) * (1.06)^8
(x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02) * (1.06)^9
(x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02) * (1.06)^10
(x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02) * (1.06)^10
(x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02)
x3 = (x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02)
x3 * 1.06
x3 * 1.06^2
x3 * 1.06^3
x3 * 1.06^4
x0
x0
x1
x2
(x2/x0)-1
x2/x1 -1
x2 * 1.06 + 200000 * 1.04 + 600000 * 1.02
x3
x3 * 1.10
x3 * 1.12
x2 * 1.10 + 200000 * 1.06 + 600000 * 1.03
install.packages("ISLR")
install.packages(c("backports", "bayestestR", "BH", "biglm", "bookdown", "boot", "brio", "broom", "car", "carData", "caret", "class", "cli", "clipr", "coda", "codetools", "colorspace", "cplm", "cpp11", "crayon", "crosstalk", "curl", "data.table", "DBI", "dbplyr", "diffobj", "digest", "dplyr", "DT", "e1071", "effects", "effectsize", "emmeans", "fansi", "fastmap", "forcats", "foreach", "Formula", "fs", "fst", "generics", "GGally", "ggplot2", "ggrepel", "ggthemes", "gh", "git2r", "glue", "gower", "hexbin", "hms", "htmltools", "htmlwidgets", "httpuv", "insight", "iterators", "jsonlite", "kableExtra", "KernSmooth", "knitr", "labeling", "later", "lattice", "lava", "lifecycle", "lme4", "lmtest", "lubridate", "magrittr", "maptools", "markdown", "MASS", "Matrix", "memoise", "mgcv", "mime", "misc3d", "mlbench", "ModelMetrics", "mvtnorm", "nlme", "nloptr", "nnet", "numbers", "openintro", "openssl", "openxlsx", "parameters", "pbkrtest", "performance", "pillar", "pkgbuild", "pkgconfig", "plotly", "plyr", "prettyunits", "pROC", "processx", "promises", "ps", "quantreg", "R6", "Rcpp", "RcppEigen", "RCurl", "readODS", "readr", "recipes", "repr", "reprex", "reshape2", "rgdal", "rlang", "rmarkdown", "ROI", "ROI.plugin.glpk", "ROI.plugin.quadprog", "rprojroot", "rsconnect", "sandwich", "scales", "selectr", "shiny", "sjlabelled", "sjmisc", "sjstats", "skimr", "slam", "sp", "spatial", "SQUAREM", "statmod", "stringi", "survey", "survival", "sys", "testthat", "tibble", "tinytex", "tufte", "tweedie", "usethis", "vctrs", "webshot", "whisker", "withr", "xfun", "yaml", "zip"))
install.packages(c("dplyr", "misc3d", "rgdal"))
install.packages("dplyr")
install.packages("dplyr")
890000 / 906000
1728 * (365 - 16) / 365
3291 -3246
library(caret)
installed.packages()
installed.packages = installed.packages()
install.packages()
install.packages
installed.packages
write(installed.packages, "R_packagelist.txt")
remove.packages()
remove.packages()
ip <- as.data.frame(installed.packages())
ip
head(ip)
View(ip)
ip2 <- subset(ip, Priority != 'base' )
ip2
View(ip2)
ip2 <- subset(ip, is.na(Priority) )
path.lib <- unique(ip2$LibPath)
path.lib
pkgs.to.remove <- ip2[,1]
pkgs.to.remove
sapply(pkgs.to.remove, remove.packages, lib=path.lib)
installed.packages()
ip <- as.data.frame(installed.packages())
ip
View(ip)
ip2 <- subset(ip, Priority=='recommended' )
ip2
pkgs.to.remove <- ip2[,1]
sapply(pkgs.to.remove, remove.packages, lib=path.lib)
ip <- as.data.frame(installed.packages())
ip
View(ip)
setwd("/Volumes/GDRIVE_SSD/Github/DATA622_MACHINELEARNING/HW3")
?md.pattern
a = mice::md.pattern(cla, rotate.names = T)
# This code chunk should only be run to build the common data set.
# Any model builder should rely on importing the common data set only.
# Thus, set eval=TRUE when generating a new version of the common data set.
# Otherwise, set eval=FALSE to skip this step.
# ------------------------------------------------------------------------
# This code block assumes the raw Loan Approval data file in csv form has been placed in the same folder as this script.
# ----------------------------------------------------------------------------------------
cla <- read_csv("Loan_approval.csv") %>% dplyr::select(-Loan_ID)  # drop the row identifier.
setwd("/Volumes/GDRIVE_SSD/Github/DATA622_MACHINELEARNING/HW3")
final_fit %>%
collect_predictions() -> pred
model_report = data.frame(
model=c("CART Decision Tree", "Random Forest", "Gradient Boost" ),
software = c("tidymodels", "VSURF", "xgboost") ,
accuracy = c( 74.7, 81.1,  86.3 ) ,
kappa = c( 36.2 , 45.5,  63.9) )
kable(model_report, digits = 1 , caption = "Model Performance on Loan Approvals") %>% kable_styling(bootstrap_options = c("striped", "hover") ) %>%
footnote( general = "All models used the same data with 80-20% split of training to test data from a
common dataset."
)
library(tidyverse)
library(kableExtra)
model_report = data.frame(
model=c("CART Decision Tree", "Random Forest", "Gradient Boost" ),
software = c("tidymodels", "VSURF", "xgboost") ,
accuracy = c( 74.7, 81.1,  86.3 ) ,
kappa = c( 36.2 , 45.5,  63.9) )
kable(model_report, digits = 1 , caption = "Model Performance on Loan Approvals") %>% kable_styling(bootstrap_options = c("striped", "hover") ) %>%
footnote( general = "All models used the same data with 80-20% split of training to test data from a
common dataset."
)
cla$Loan_Status
cla
View(df2)
# Your libraries go here
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(caret)    # Model Framework
library(skimr)    # Used for EDA
library(klaR)     # Implemented KNN and Naive Bayes models, etc
library(class)    # used for KNN classifier
# PLEASE ADD YOUR R LIBRARIES BELOW
# ------------------------------
library(tidymodels)
library(VSURF)
library(dplyr)
library(mice)
library(vip)
library(reshape2) # PT
library(data.table) # PT
library(xgboost)
# ---------------------------------
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
# Alex:
# This code chunk is only to be used for conditionally disabling certain
# very slow model code as I merge different sections and need to frequent recompile
runSlowChunks = F
library(palmerpenguins)
# The final dataset excludes observations with missing sex and drops the island and year variables.
pc = penguins %>% filter( is.na(sex) == FALSE) %>% dplyr::select( -one_of("island", "year") )
head(pc) %>% kable(caption="Scrubbed penguin data set") %>% kable_styling(bootstrap_options = c("striped", "hover"))
set.seed(10)
#  Construct the standardized dataset with the response variable, the 4 quantitative variables normalized
#  with the scale function and the sex variable normalized with the 1,-1 assignment explained previously.
#
standardized = cbind( pc[,1], scale( pc[, 2:5] ) , sex = data.table::fifelse(pc$sex == 'male' , 1 , -1) )
# Define an 80-20 split of the training and test data.
# -------------------------------------------------------------------------------------
training.individuals = createDataPartition(standardized$species, p= 0.8 , list = FALSE)
# X variables include bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g,  sex (converted to -1,1)
# Y variable is just the species class
# train data is the standardized subset of the full data set
# test data is the complement of the training data set.
# -----------------------------------------------------------------
train.X  =  standardized[ training.individuals,  2:6]
test.X   =  standardized[-training.individuals,  2:6]
train.Y  =  standardized[ training.individuals,    1]
test.Y   =  standardized[-training.individuals,    1]
head(train.X ) %>% kable(caption="Normalized Training Data - 1st Few Rows", digits = 2 ) %>%
kable_styling(bootstrap_options = c("striped", "hover") )
model = train( species ~. , data = standardized , method = "knn" ,
trControl = trainControl("cv", number = 5 ), tuneLength = 10 )
# Plot the model output vs. varying k values.
plot(model)
model
# The best tuning parameter k that
# optimized model accuracy is:
#model$bestTune
# Simpler interface using the knn function within class.
knn.pred5 = knn(train.X, test.X, train.Y , k = 5, prob = FALSE)
cm5 = confusionMatrix(knn.pred5, test.Y)
# This model approach uses tuneGrid with a single value to calculate kNN
# only at a single number of nearest neighbors.
# ------------------------------------------------------------------------------
x2 = train( species ~ . , data = standardized[training.individuals,] ,
method = "knn",
trControl = trainControl(method="none"),
tuneGrid=data.frame(k=5) )
pred2 = predict(x2, newdata=test.X)
(cmknn2=confusionMatrix(pred2, test.Y))
# We visualize a projection of the decision boundary flattened to 2 dimensions to illustrate
# the smoothness of the KNN boundary at different values of k.
# First, choose the variables, then build a sampling grid.
# ------------------------------------------------------------------------
pl = seq(min(standardized$bill_length_mm), max(standardized$bill_length_mm), by = 0.05)
pw = seq(min(standardized$bill_depth_mm), max(standardized$bill_depth_mm), by = 0.05 )
# The sampling grid needs explicit values to be plugged in for the variables
# which are not the two projected variables.  Since variables are standardized to
# mean 0 and variance 1, we choose zero for flipper_length and body_mass because they are continuous
# and the mean and mode are close.
# However, for sex, we choose male because the average sex = 0 is not the mode.
# In practice, sex = 0 gives weird decision boundaries in the projected plot.
# ------------------------------------------------------------------------------
lgrid = expand.grid(bill_length_mm=pl , bill_depth_mm = pw , flipper_length_mm = 0, body_mass_g = 0 , sex = 1 )
knnPredGrid = predict(x2, newdata=lgrid )
num_knnPredGrid = as.numeric(knnPredGrid)
num_pred2 = as.numeric(pred2)
test = cbind(test.X, test.Y)
test$Pred = num_pred2
ggplot(data=lgrid) + stat_contour(aes(x=bill_length_mm, y= bill_depth_mm, z = num_knnPredGrid), bins = 2 ) +
geom_point( aes(x=bill_length_mm, y= bill_depth_mm, color = knnPredGrid),  size = 1, shape = 19, alpha = 0.2) +
geom_point( data= test , aes(x=bill_length_mm, y=bill_depth_mm, color=pred2 ) , size = 4, alpha = 0.8, shape =24 ) +
ggtitle("KNN Decision Boundary for Penguins Data with k=5 neighbor")
# This model approach uses tuneGrid with a single value to calculate kNN
# only at a single number of nearest neighbors.
# ------------------------------------------------------------------------------
x3 = train( species ~ . , data = standardized[training.individuals,] ,
method = "knn",
trControl = trainControl(method="none"),
tuneGrid=data.frame(k=1) )  # Now we use a k=1 nearest neighbor algorithm which is overfitted.
pred3 = predict(x3, newdata=test.X)
knnPredGrid3 = predict(x3, newdata=lgrid )
num_knnPredGrid3 = as.numeric(knnPredGrid3)
num_pred3 = as.numeric(pred3)
test = cbind(test.X, test.Y)
test$Pred = num_pred3
ggplot(data=lgrid) + stat_contour(aes(x=bill_length_mm, y= bill_depth_mm, z = num_knnPredGrid3), bins = 2 ) +
geom_point( aes(x=bill_length_mm, y= bill_depth_mm, color = knnPredGrid3),  size = 1, shape = 19, alpha = 0.2) +
geom_point( data= test , aes(x=bill_length_mm, y=bill_depth_mm, color=pred3 ) , size = 4, alpha = 0.8, shape =24 ) +
ggtitle("KNN Decision Boundary for Penguins Data with k=1 neighbor")
# This code chunk should only be run to build the common data set.
# Any model builder should rely on importing the common data set only.
# Thus, set eval=TRUE when generating a new version of the common data set.
# Otherwise, set eval=FALSE to skip this step.
# ------------------------------------------------------------------------
# This code block assumes the raw Loan Approval data file in csv form has been placed in the same folder as this script.
# ----------------------------------------------------------------------------------------
cla <- read_csv("Loan_approval.csv") %>% dplyr::select(-Loan_ID)  # drop the row identifier.
# Add a column for Total Income of Applicant and Co-Applicant
# ---------------------------------------------------------------------------
cla$Total_Income = cla$ApplicantIncome + cla$CoapplicantIncome
a = mice::md.pattern(cla, rotate.names = T)
# We build a dataset in which all observations have fully populated values.
# ---------------------------------------------------------------------------
cla = na.omit(cla)
# Add mean zero and variance 1 versions of quantitative variables.
# ----------------------------------------------------------
cla %>% mutate( sApplicantIncome = scale(ApplicantIncome),
sCoapplicantIncome = scale(CoapplicantIncome),
sTotal_Income = scale(Total_Income) ,
sLoanAmount   = scale( LoanAmount)
) -> cla
head(cla)
write_csv(cla, "cla.csv") # Write the scaled common loan approvals data set to local disk.
cla = read_csv("cla.csv")
# Transform the character data columns into factors
# --------------------------------------------------------
cla$Loan_Amount_Term = factor(cla$Loan_Amount_Term)
cla$Loan_Status = factor(cla$Loan_Status)  # Convert the response to factor
cla$Credit_History = factor(cla$Credit_History)
cla$Property_Area = factor( cla$Property_Area)
cla$Gender = factor( cla$Gender)
cla$Married = factor(cla$Married)
cla$Dependents = ordered(cla$Dependents, levels = c("0" , "1", "2" , "3+") )
cla$Education = factor(cla$Education)
cla$Self_Employed = factor( cla$Self_Employed)
ncla <- cla[,1:13]
GGally::ggpairs(ncla, mapping = "Loan_Status", columns = c(6:8, 13, 12)) %>% print()
GGally::ggpairs(ncla, mapping = "Loan_Status", columns = c(1:5, 10:12), ) %>% print()
#  Display the approval rate statistics for each qualitative field
#  in absolute count and percentage terms by category.
#
display_approval_rate <- function(la , field )
{
a = la[, "Loan_Status"]
b = la[, field]
c = table(cbind(a,b))  # Count table
pcts = as.data.frame.matrix(c) /nrow(la) * 100   # Percentage equivalent
col_sums = colSums(pcts)  # Shows the results by the field
app_rate = pcts[2,]/col_sums   * 100   # Approval rate assumes row 2 is the "Yes" row.
yy = rbind( pcts, col_sums, app_rate, colSums(c))
rownames(yy)[1]= paste0( rownames(yy)[1], " (%)")
rownames(yy)[2]= paste0( rownames(yy)[2], " (%)")
rownames(yy)[3]= paste0( field, " Column Total %:")
rownames(yy)[4]="Loan Approval Rate by Column%"
rownames(yy)[5]="Total Applications:"
print(yy %>% kable(digits = 1, caption = paste0('Loan Approved vs. ', field , ' by Percent') ) %>%
kable_styling(bootstrap_options = c("striped", "hover") )  %>%
row_spec(4, background = "skyblue", bold = TRUE) )
}
cat_vars = c("Property_Area", "Credit_History", "Loan_Amount_Term", "Self_Employed", "Education", "Dependents", "Married", "Gender")
for( field in cat_vars)
{
display_approval_rate( cla , field)
cat("\n")
}
set.seed(10)
data_split <- initial_split(cla, prop = .8)
train_data <- training(data_split)
test_data  <- testing(data_split)
use_data <- train_data %>%dplyr::select( -ApplicantIncome, -CoapplicantIncome, -Total_Income, -sApplicantIncome, -sCoapplicantIncome, -LoanAmount)
surfedModel <- invisible(VSURF::VSURF(use_data%>%dplyr::select(-Loan_Status), use_data$Loan_Status, parallel=F))
View(ts)
vsurfStages <- tibble('Stage' = "Prediction" , 'Fields' =colnames(use_data%>%dplyr::select(-Loan_Status))[surfedModel$varselect.pred])
vsurfStages <-vsurfStages %>% add_row('Stage' = 'interpretation' , 'Fields'=paste(colnames(use_data%>%dplyr::select(-Loan_Status))[surfedModel$varselect.interp], collapse=", "))
vsurfStages <-vsurfStages %>% add_row('Stage' = 'Ranked Importance' , 'Fields'=paste(colnames(use_data%>%dplyr::select(-Loan_Status))[surfedModel$varselect.thres], collapse = ", "))
vsurfStages
loans_testing <- na.omit(testing(data_split))
loans_testing["factorCreditHistory"] <- as.factor(as.logical((loans_testing)$Credit_History==1))
loans_testing["factorLoanStatus"] <- as.factor((loans_testing)$Loan_Status=="Y")
loans_testing %>% metrics(truth = factorLoanStatus, estimate = factorCreditHistory)
loan_testing <- na.omit(testing(data_split))
loan_testing["factorCreditHistory"] <- as.factor(as.logical((loan_testing)$Credit_History==1))
loan_testing["factorLoanStatus"] <- as.factor((loan_testing)$Loan_Status=="Y")
actual_result <- loan_testing %>% metrics(truth = factorLoanStatus, estimate = factorCreditHistory)
ourAccuracy <- tibble('Variables' = 1 , 'Kappa' =actual_result$.estimate[2], 'Accuracy' = actual_result$.estimate[1])
ourAccuracy <- ourAccuracy[-c(1),]
#loan_testing <- testing(data_split) #reset with nas added back in
for(var in 1:length(surfedModel$varselect.thres)){
loans_rf_rec <-  recipe(Loan_Status~., data = train_data)  %>%
step_rm(everything(),-all_of(colnames(use_data[(surfedModel$varselect.thres)[1:var]])), -Loan_Status)  %>%
#step_knnimpute(all_predictors()) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
prep()
loan_training<-juice(loans_rf_rec)
loan_ranger <- rand_forest( mode = "classification") %>%   set_engine("ranger") %>%     fit(Loan_Status ~ ., data = loan_training)
loan_testing <- loans_rf_rec  %>% bake(testing(data_split))
actual_result <- loan_ranger %>% predict(loan_testing) %>%     bind_cols(loan_testing) %>%     metrics(truth = Loan_Status, estimate = .pred_class)
ourAccuracy <- ourAccuracy %>% add_row('Variables' = var , 'Kappa' =actual_result$.estimate[2], 'Accuracy' = actual_result$.estimate[1])
}
ourAccuracy %>% ggplot(aes(x=Variables,y=Accuracy)) + ggtitle("Accuracy as variables are added") +geom_point(color="red") + ylim(0,1) + scale_color_viridis_d(option = "plasma", begin = .9, end=0)
ourAccuracy %>% ggplot(aes(x=Variables,y=Kappa)) +ggtitle("Kappa as Variables are added") +geom_point(color="blue") + ylim(0,.6) + scale_color_viridis_d(option = "plasma", begin = .9, end=0)
ourAccuracy
our_data<-use_data[,colnames(use_data[(surfedModel$varselect.thres)])]
our_data$Loan_Status<-use_data$Loan_Status
loans_rf_rec <-  recipe(Loan_Status~., data = our_data)  %>%
prep()
loans_grid<-grid_regular(trees(), finalize(mtry(), use_data), levels=5)
loan_training<-juice(loans_rf_rec)
loan_ranger <- workflow()%>% add_recipe(loans_rf_rec) %>% add_model(rand_forest(mtry=tune::tune(), trees = tune::tune(), mode = "classification") %>% set_engine("ranger",importance = "impurity"))
folds <- vfold_cv(our_data)
class_metrics <- metric_set( kap, accuracy)
cla$Loan_Status
cla$Loan_Status=='Y'
count(cla$Loan_Status=='Y')
sum(cla$Loan_Status=='Y')
sum(cla$Loan_Status=='Y')/nrow(cla)
1-sum(cla$Loan_Status=='Y')/nrow(cla)
