---
title: "HW3tree"
author: "Randall Thompson"
date: "3/23/2021"
output: html_document
---
```{r}
library(tidyverse)
library(tidymodels)

cla = read_csv("cla.csv")

cla$Loan_Amount_Term = factor(cla$Loan_Amount_Term)
cla$Loan_Status = factor(cla$Loan_Status)  # Convert the response to factor
cla$Credit_History = factor(cla$Credit_History)
cla$Property_Area = factor( cla$Property_Area)
cla$Gender = factor( cla$Gender)
cla$Married = factor(cla$Married)
cla$Dependents = ordered(cla$Dependents, levels = c("0" , "1", "2" , "3+") )
cla$Education = factor(cla$Education)
cla$Self_Employed = factor( cla$Self_Employed)
```

After we convert our text variables to factors, let's take a look at the shape and completeness of our data. 

```{r}
skimr::skim(cla)
```

Rows with missing data has been removed. Five of our independant variables have only two levels as displayed in n_unique. For our dependant variable, Loan_Status, we have twice the number of yes's than no's. We could downsample to have an even split of yes/no but with only, 480 rows left, we decided we didn't want to lose any more data. The four numeric variables have been duplicated then centered to a mean of 0 and scaled to a standard deviation of 1. Since out CART algorithm uses sum of squares to select splits and since center and scaled data shouldn't effect the relationship between variables, I'm going to stick with the original currency values. 

```{r}
ncla <- cla[,1:13]
```


Next we explore the pairwise correlation of numeric and factor variable separately.

```{r}
GGally::ggpairs(ncla, mapping = "Loan_Status", columns = c(6:8, 13, 12)) %>% print()
```
The pairwise correlation plot shows statistical significants with three asterisks next to the correlation value. As expected, Total Income is highly correlated with applicant and coapplicant incomes since Total income is a sum of the two. More importatntly, Loan Amount is moderately positively correlated with Applicant Income and weakly positively correlated with Coapplicant Income. Overall, there doesn't appear to be a large difference in distribution among numeric variables between a loan being approved or not.

```{r}
GGally::ggpairs(ncla, mapping = "Loan_Status", columns = c(1:5, 10:12), ) %>% print()
```

For pairwise factor bar charts, you're looking for large mismatches in box size within the upper triangle of the matrix. For example: number of dependants vs married. The top row of boxes look fairly even meaning married people have a fairly even distribution of number of dependants. However, non-married people have a high level of 0 dependants and not very many 1's, 2's, or 3+'s. Our varialbe of interest Loan_status has a large mismatch on Credit_history. There appears to be an even amount of people who have and don't have credit history when the loan status is not approved but almost no one has a loan status approved without also haveing a credit history of yes. This is quantified in our earlier analysis. This large variance should make credit history a good predictor variable.

Now we create our model recipe. This recipe object describes the dependant and independant variables we wish to use and the data source. 
```{r}
loan_rec <- recipe(Loan_Status~., data=ncla) %>% 
  prep()

loan_rec
```

Next we define the statistical model we wish to run on our recipe object. In this case, we are running a decision tree from the rpart package. The mode of decision tree we select is classification. Classification in the rpart package will split branches in a way that minimizes the sum of squares error.
```{r}
tree <- decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tree
```

Now we create our training and test splits using an 80/20 split. The training set will be used to train our model while the test split will be used to evaluate our final model.
```{r}
set.seed(1234)
loan_split <- initial_split(ncla, prop = .8)

loan_train <- training(loan_split)
loan_test <- testing(loan_split)

loan_split
```

Now we use our model engine and our training data to create a classification model.
```{r}
tree_fit <- tree %>% fit(Loan_Status~., data = loan_train)

tree_fit 
```

Our top level splits are credit history, property area, total income, then loan amount. Our first branch is also a terminal node. When credit history = 0, you are very unlikely to be approved for a loan. 

No we're going to resample our training data with 10 crossfold validation by taking a random 90% and checking the variability of our estimates. 
```{r}
set.seed(1234)
loan_xval <- vfold_cv(data = loan_train, strata = Loan_Status) 
```

Here we apply our resampled data to our model and define the metrics of interest.
```{r}
tree_res <- fit_resamples(
  tree,
  loan_rec,
  resamples = loan_xval, 
  metrics = metric_set(accuracy, kap, roc_auc, sens, spec),
  control = control_resamples(save_pred = TRUE)) 

tree_res %>%  collect_metrics(summarize = TRUE)
```

Our prediction accuracy is 73%. To maximize accuracy, we are going to select the model that will optimize for accuracy. This will be the model we use in our final workflow. We fit our total training data one last time and view our splits. 

```{r}
best_tree <- tree_res %>%
  select_best("accuracy")

final_wf <- 
  workflow() %>% 
  add_model(tree) %>% 
  add_recipe(loan_rec) %>%
  finalize_workflow(best_tree)

final_tree <- 
  final_wf %>%
  fit(data = loan_train)

final_tree
```

Our branches and nodes do not appear to have changed. Let's look at variable importance. Importance in a decision tree is a combination of number of times a variable appears in a branch, how high of a branch, and number of times it appears in a terminal node. 

```{r}
final_tree %>% 
  pull_workflow_fit() %>% 
  vip::vip()
```

Credit History is by far our most importable variable for predicting loan status. 

Lastly, we test our model and collect our metrics. 

```{r}
final_fit <- 
  final_wf %>%
  last_fit(split = loan_split, metrics = metric_set( accuracy, kap, roc_auc, sens, spec))

final_fit %>%
  collect_metrics()
```

Our accuracy is 80%. Below is our confusion matrix. 

```{r}
final_fit %>%
  collect_predictions() %>% 
  conf_mat(Loan_Status, .pred_class)
```

