---
title: "Data 622 Homework 4: Mental Health Data Modeling"
author: "Group 6: Alexander Ng, Scott Reed, Philip Tanofsky, Randall Thompson"
date: "Submitted by 05/07/2021"
output:
  html_document:
    df_print: paged
    highlight: pygments
    number_sections: yes
    theme: readable
    toc: yes
    toc_depth: 2
    toc_float: no
    fontsize: 12
  pdf_document:
    toc: yes
    toc_depth: '3'
    number_sections: true
    df_print: kable
    highlight: tango
editor_options:
  chunk_output_type: inline
fontsize: 11pt
urlcolor: blue
---

# Prompt

For this assignment, we will be working with a very interesting mental health data set from a real-life research project. All identifying information, of course, has been removed. The attached spreadsheet has the data (the tab name "Data"). The data dictionary is given in the second tab. You can get as creative as you want. The assignment is designed to really get you to think about how you could use different methods.

1. Please use a clustering method to find clusters of patients here. Whether you choose to use k-means clustering or hierarchical clustering is up to you as long as you reason through your work. Can you come up with creative names for the profiles you found? (60)

2. Let's explore using Principal Component Analysis on this data set. You will note that there are different types of questions in the data set: column: E-W: ADHD self-report; column X â€“ AM: mood disorders questionnaire, column AN-AS: Individual Substance Misuse; etc. Please reason through your work as you decide on which sets of variables you want to use to conduct Principal Component Analysis. (60)

3. Assume you are modeling whether a patient attempted suicide (columnAX). Please use support vector machine to model this. You might want to consider reducing the number of variables or somehow use extracted information from the variables. This can be a really fun modeling task! (80)

```{r warning=F, message=F}
# Import required R libraries
library(kernlab)
library(caret)
theme_set(theme_classic())

library(tidyverse)
library(tidymodels)
library(skimr)
library(vip)
library(rpart.plot)
```

# Data Extraction and Variable Selection

In researching the specifics of the attention deficit hyperactivity disorder (ADHD) questionnaire (https://add.org/wp-content/uploads/2015/03/adhd-questionnaire-ASRS111.pdf) and the mood disorder (MD) questionnaire (https://www.ohsu.edu/sites/default/files/2019-06/cms-quality-bipolar_disorder_mdq_screener.pdf), derived variables were created and assessed in an attempt to better define the diagnosis of the patient providing the survey answers. Through trial and error the mood disorder questionnaire provided value in determining a suicide attempt. According to the survey explanation, an answer of Yes (1) to seven or more events in question 1, a Yes answer to question 2 and finally a two or three response to question 3 warrants a recommendation for further medical assessment for bipolar disorder. Based on this relationship in the question, a count of the Yes answers to question 1 is calculated. That calculation along with the results of question two and question three are used as inputs to the SVM model. Based on the PCA, abuse and sex were also identified as significant to the determination of suicide attempt. The ADHD total also showed significance to the SVM model, but after several trials, the ADHD total variable was removed as the variable did not improve the model test results.

Create a variable for the count of Yes answers to the events from MD question 1. Also, Sex, Abuse, and MD.Q2 are defined as factor variables instead of character variables. Finally, the dependent variable, Suicide, is also defined as a factor data type. The SVM model requires character or factor variables along with numeric variables.

```{r warning=F, message=F}
data <- read_csv("New_adhd.csv")

data$MD.Q1.Count <- 0
data$MD.Q1.Count <- ifelse(data$MD.Q1a == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1b == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1c == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1d == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1e == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1f == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1g == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1h == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1i == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1j == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1k == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1L == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)
data$MD.Q1.Count <- ifelse(data$MD.Q1m == 1, data$MD.Q1.Count+1, data$MD.Q1.Count)

data$fSex <- as.factor(data$fSex)
data$fSuic <- as.factor(data$fSuic)
data$fAbuse <- factor(data$fAbuse, levels=c("0","1","2","3","4","5","6","7"), ordered=TRUE)
data$MD.Q2 <- as.factor(data$MD.Q2)

skim(data)
```

Based on the final imputed data set, a total of 171 instances exist with Suicide attempts accounting for 49 of the total.

```{r warning=F, message=F}
# of the 171, Suicide Yes=49, Suicide No=122
table(data$fSuic)
```

The data set is initially split into a training and test with 75% of the data used in the training set which equals 129 of the 171 overall instances. The test set consists of 42 instances.

```{r warning=F, message=F}
# Split into training and test data
set.seed(123)
# split the data into training (75%) and testing (25%)
data_split <- initial_split(data, prop = .75)
data_split

data_train <- training(data_split)
data_test <- testing(data_split)

data_train
```

```{r warning=F, message=F}
# Create CV object from training data
data_cv <- vfold_cv(data_train, v=5, repeats=3)
```

# Model Definition

Using tidymodels, the recipe function is used to define the independent variables, Sex, Abuse, MD.Q1.Count, MD.Q2, and Md.Q3 in relationship to the dependent variable, Suicide. In order to properly prepare the numeric and factor data, the recipe includes a step to normalize the numeric data and create dummy variables for the factor data. These steps will ensure proper balance in the measurements of the SVM model.


```{r warning=F, message=F}
# https://www.tidymodels.org/learn/work/tune-svm/
# 0.8571429 accuracy
data_recipe <- recipe(fSuic ~ fSex + MD.Q1.Count + MD.Q2 + MD.Q3 + fAbuse, data = data) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes())  
```

In order to evaluate the different versions of the SVM models defined, the metrics of ROC AUC, accuracy, and kappa are used.

```{r warning=F, message=F}
# using roc, accuracy and kappa
roc_vals <- metric_set(roc_auc, accuracy, kap)
```

```{r warning=F, message=F}
# Verbose turned off, save out-of-sample predictions is turned on
ctrl <- control_grid(verbose=F, save_pred=T)
```


```{r warning=F, message=F, eval=F}
# SVM Model with Polynomial
svm_model <- svm_poly(cost = tune(),
                      degree = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification") %>% 
  translate()
```


Given the test data set contains 42 instances, and with the current seed value, 31 of the 42 are No for Suicide. By simply picking No, for every selection, an accuracy of 73.8% would be achieved. This accuracy gives a baseline in which to find a more accurate model.

With tidymodels, the SVM model is available using a polynomial kernel or a radial basis function (RBF) kernel. The RBF kernel approach was selected as the function measures the distance between the feature vectors. The kernel value decreases with distance and measures on a scale from zero to one. With the small size of the data set and limited number of variables, the RBF kernel outperformed the polynomial kernel option.

The tidymodels approach allowed for parameter tuning, which in the case of the RBF kernel, the two parameters to the model are cost and rbf_sigma.

- cost: The cost of predicting an instance within or outside of the margin

- rbf_sigma: The precision parameter for the radial basis function

With the opportunity to tune the parameters, a cross-fold validations setting of five folds and three repeats, resulting in 15 separate model combinations proved the most effective in terms of the final result while also keeping the model as simple as possible.

```{r warning=F, message=F}
# SVM Model with Radial Basis Function
svm_model <- svm_rbf(cost = tune(),
                     rbf_sigma = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab") %>% 
  translate()
```

```{r warning=F, message=F}
# Generate formula
formula_res <- svm_model %>%
  tune_grid(data_recipe,
    resamples = data_cv,
    metrics = roc_vals,
    control = ctrl
  )

#formula_res
```

# Model Evaluation

The parameter tuning ran a five-fold cross-validation repeated three times. The marginal plot shows each predictor plotted against performance. Both parameters are presented in different log scales to ensure the values along the x-axis are spread evenly for display purposes. As the plot shows, the performance of the two parameters based on the ROC AUC is consistently above 0.650 with several points above 0.700.

The output of the raw metrics shows very poor performance for kappa, with only a couple tuned models resulting in a value above zero. The best results for accuracy show a consistent 0.705, which realistically represents the selection of "No" for every instance.

Finally, the output for the best ROC AUC indicates good performance with the top five results greater than 0.67 and indicate a potential varying selection method as compared to the accuracy metric.

```{r warning=F, message=F}
# Execute with a recipe
set.seed(123)
recipe_res <- svm_model %>%
  tune_grid(data_recipe,
    resamples = data_cv,
    metrics = roc_vals,
    control = ctrl
  )
#recipe_res

# https://www.tidyverse.org/blog/2020/07/tune-0-1-1/
autoplot(recipe_res, metric="roc_auc") +
  ggtitle("Parameter Tuning Results")
```


```{r warning=F, message=F}
# Display top combinations
show_best(recipe_res, metric = "kap")
```

```{r warning=F, message=F}
# Display top combinations
show_best(recipe_res, metric = "accuracy")
```

```{r warning=F, message=F}
# Best setting
show_best(recipe_res, metric = "roc_auc")
```


```{r warning=F, message=F}
# Set the workflow
svm_wf <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(svm_model)
```

Based on previous modeling attempts, the kappa metric was used to determine the best tuned model before performing the final predictions. As the kappa results consistently resulted in low values, the decision was made to base the model selection on the ROC AUC metric. In repeated model attempts, the ROC AUC metric proved better at selecting a model with higher accuracy on the test data set. The SVM model based on the RBF kernel with the best ROC AUC contains a cost parameter of 0.501 and an rbf_sigma value of 0.0426.

```{r warning=F, message=F}
# Select best model based on roc_auc
best_svm <- recipe_res %>%
  select_best(metric = 'roc_auc')

# view the best svm parameters
best_svm
```

```{r warning=F,message=F}
# finalize workflow
final_svm_wf <- svm_wf %>%
  finalize_workflow(best_svm)
```

The summary of the final SVM model object reiterates the findings and decisions previously described. An SVM model based on the RBF kernel with two recipe steps: normalization of numeric data and creation of dummy variables for factor data. The SVM model will perform classification with a cost parameter value of 0.5007 and an RBF sigma parameter value of 0.0426. The model contains 83 support vectors.

```{r warning=F,message=F}
# fit the model
svm_wf_fit <- final_svm_wf %>%
  fit(data = data_train)

#https://stackoverflow.com/questions/62772397/integration-of-variable-importance-plots-within-the-tidy-modelling-framework
svm_wf_fit
```

From the final model fit using the tidymodels workflow process, the fit of the selected model on the trained data results in an accuracy of 85.7% and an ROC AUC of 87.5%.

```{r warning=F,message=F}
# train and evaluate
svm_last_fit <- final_svm_wf %>%
  last_fit(data_split)

svm_last_fit %>% collect_metrics()
```

# Model Predictions

The ROC AUC of the train and test plot indicate the curves for each data split. Based on the plot, the test data appears to outperform the training data. This result shows overfitting did not occur on the training data as often is the case in model creation.

```{r warning=F,message=F}
# https://fahadtaimur.wordpress.com/2020/07/19/tuning-svm-in-r-2/
scored_train <- predict(svm_wf_fit, data_train, type="prob") %>%
    bind_cols(predict(svm_wf_fit, data_train, type="class")) %>%
    bind_cols(.,data_train)

scored_test <- predict(svm_wf_fit, data_test, type="prob") %>%
      bind_cols(predict(svm_wf_fit, data_test, type="class")) %>%
      bind_cols(., data_test) 

scored_train %>%
  mutate(model = "train") %>%
  bind_rows(scored_test %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(fSuic, .pred_No) %>%
  autoplot() %>%
    print()

```

For clarity, the confusion matrix of the test data predictions indicates a total of 36 correct selections of the 42 instances. As noted previously, a prediction of "No" for every instance results in an accuracy of 73.8%. This model clearly outperforms the baseline model with an accuracy greater than 85%. The ability of the model to correctly predict 6 "Yes" instances shows the model truly created a maximum margin between the variables in the vector planes.  

```{r warning=F,message=F}
svm_predictions <- svm_last_fit %>% collect_predictions()

conf_mat(svm_predictions, truth = fSuic, estimate = .pred_class)

svm_predictions

mean(svm_predictions$.pred_class == svm_predictions$fSuic)
```

The density plot shows the frequency of the predicted "Yes" values across the test data set. The plot does indicate for those true values of "Yes", the density was quite consistent across the predicted "Yes" values. Given this realization, the ability to identify 6 of the 11 true "Yes" results indicates the model can be relied upon to some degree in identifying individuals who have attempted suicide. With the large spike in density for true "No" instances, the model does not suffer in predicting "No" values. Given the propensity of the tuned models to simply always predict "No", the final tuned model achieved success in utilizing the SVM method to properly separate instances based on the classification approach.

```{r warning=F, message=F}
svm_predictions %>%
  ggplot() +
  geom_density(aes(x = .pred_Yes, fill = fSuic),
               alpha = 0.5)
```

The final five plots display the performance of the predicted "Yes" value against the individual independent variables separated by the true values for Suicide Attempt.

- The MD.Q1.Count plot shows the values of seven or greater are a reasonable predictor of Suicide Attempt and the SVM model reflects the same.

- The MD.Q2 plot indicates that for a true value of Yes, the variable value is more likely to be 1. A variable value of 1 does not directly indicate a suicide attempt, but a value of 0 very likely indicates no suicide attempt.

- The MD.Q3 plot shows a similar understanding as MD.Q1.Count, in which, for those with a suicide attempt, the value is more likely to be higher, 2 or 3 for this variable.

- The fSex plot does show for suicide attempt a higher volume of female instances. Assessing the true No side of the plot, male instances due result in low predicted Yes values.

- The fAbuse plot clearly shows for those with higher levels of abuse, a predicted "Yes" can result.

```{r warning=F, message=F}
# https://www.tidymodels.org/learn/work/tune-svm/
augment(recipe_res) %>%
  ggplot(aes(MD.Q1.Count, .pred_Yes, color = fSuic)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~fSuic)

augment(recipe_res) %>%
  ggplot(aes(MD.Q2, .pred_Yes, color = fSuic)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~fSuic)

augment(recipe_res) %>%
  ggplot(aes(MD.Q3, .pred_Yes, color = fSuic)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~fSuic)

augment(recipe_res) %>%
  ggplot(aes(fSex, .pred_Yes, color = fSuic)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~fSuic)

augment(recipe_res) %>%
  ggplot(aes(fAbuse, .pred_Yes, color = fSuic)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~fSuic)
```



http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/
https://stackoverflow.com/questions/62772397/integration-of-variable-importance-plots-within-the-tidy-modelling-framework
https://github.com/tidymodels/parsnip/issues/311
https://www.tidymodels.org/learn/work/tune-svm/
https://stackoverflow.com/questions/8287344/cannot-plot-graph-for-an-svm-model-in-r
https://stackoverflow.com/questions/23613952/support-vector-machine-visualization-in-r
https://stackoverflow.com/questions/1142294/how-do-i-plot-a-classification-graph-of-a-svm-in-r